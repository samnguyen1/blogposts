---
title: 'A Short List of Important Research Papers to Understand LLM'
date: '2024-02-20'
tags: ['research', 'papers', 'LLM']
---

# A Short List of Important Research Papers to Understand LLM

<br></br>

## Introduction

<br></br>

Large Language Models (LLMs) - transformers - have revolutionised NLP and beyond. Their impact spans computer vision, computational biology, and more, showcasing the tech's versatility.

<br></br>

These models have advanced beyond simple chatbots, aiding in understanding complex protein structures and enhancing image recognition. Hereâ€™s a curated, chronological list of crucial papers for understanding LLMs/transformers, each with a brief why-it's-important note.

<br></br>

<ol>
<li>

1. **Neural Machine Translation by Jointly Learning to Align and Translate (2014)**  
   By Bahdanau, et al.  
   [Read the paper](https://arxiv.org/abs/1409.0473)  
   - Introduced attention mechanisms, changing sequential data processing.

</li>
<li>

2. **Attention Is All You Need (2017)**  
   By Vaswani, et al.  
   [Read the paper](https://arxiv.org/abs/1706.03762)  
   - Laid the groundwork for transformer models, revolutionising NLP.

</li>
<li>

3. **Universal Language Model Fine-tuning for Text Classification (2018)**  
   By Howard, et al.  
   [Read the paper](https://arxiv.org/abs/1801.06146)  
   - Introduced effective transfer learning techniques for NLP tasks.

</li>
<li>

4. **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2018)**  
   By Devlin, et al.  
   [Read the paper](https://arxiv.org/abs/1810.04805)  
   - Revolutionised contextual word embeddings, improving language understanding benchmarks.

</li>
<li>

5. **Improving Language Understanding by Generative Pre-Training (2018)**  
   By Radford, et al.  
   [Read the paper](https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035)  
   - Demonstrated the power of generative pre-training for language understanding.

</li>
<li>

6. **BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension (2019)**  
   By Lewis, et al.  
   [Read the paper](https://arxiv.org/abs/1910.13461)  
   - Introduced BART, enhancing NLP tasks with denoising for pre-training.

</li>
<li>

7. **On Layer Normalization in the Transformer Architecture (2020)**  
   By Xiong, et al.  
   [Read the paper](https://arxiv.org/abs/2002.04745)  
   - Explored layer normalisation's role in improving transformer models' stability and performance.

</li>
<li>

8. **Language Models are Few-Shot Learners (2020)**  
   By Brown, et al.  
   [Read the paper](https://arxiv.org/abs/2005.14165)  
   - Showcased GPT-3's few-shot learning capabilities, setting new benchmarks for model versatility.

</li>
<li>

9. **Scaling Language Models: Methods, Analysis & Insights from Training Gopher (2022)**  
   By Rae, et al.  
   [Read the paper](https://arxiv.org/abs/2112.11446)  
   - Discussed scaling challenges and solutions, providing insights from training the Gopher model.

</li>
<li>

10. **Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond (2023)**  
    By Yang, et al.  
    [Read the paper](https://arxiv.org/abs/2304.13712)  
    - Offers a comprehensive survey on applying LLMs like ChatGPT in various domains, highlighting practical implications and future potential.

</li>
</ol>

This list represents a journey through the evolution of LLMs, highlighting key milestones that have shaped the current landscape of AI and NLP.
